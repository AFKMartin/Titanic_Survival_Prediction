{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f7883b2",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for exploration and visualization\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# import sklearn and xgboost libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aeddbf",
   "metadata": {},
   "source": [
    "### Data exploration ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff821a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use panda to read the csv, using the head() method you can check the first 5 rows\n",
    "train = pd.read_csv(\"../data/raw/train.csv\")\n",
    "test = pd.read_csv(\"../data/raw/test.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac349265",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()\n",
    "# The .info() method gives a quick summary of the df structure.\n",
    "# In this case, the dataset has 891 rows and 12 columns.\n",
    "# For each column, it shows:\n",
    "#   - The column index (position in the df)\n",
    "#   - The column name\n",
    "#   - The number of non-null (non-missing) values\n",
    "#   - The data type (dtype) of the column\n",
    "# This information is useful for:\n",
    "#   - Detecting missing values\n",
    "#   - Understanding data types before cleaning or transformation\n",
    "#   - Getting a sense of the dataset size and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e744655",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe() # describe gives you statistical summaries of the df\n",
    "# count\tNumber of non-missing values\n",
    "# mean  -   Average value\n",
    "# std   -   Standard deviation (spread of data)\n",
    "# min   -\tMinimum value\n",
    "# 25%   -\t1st quartile (25% of data ≤ this value)\n",
    "# 50%   -   Median (middle value)\n",
    "# 75%   -\t3rd quartile (75% of data ≤ this value)\n",
    "# max   -\tMaximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7222db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe(include='object')\n",
    "# you can use this to include objects to get info in the non-numeric columns\n",
    "# unique    -   number of distinct values\n",
    "# top       -   most common value\n",
    "# freq      -   how many times that top value appears   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using describe() shows that most passengers did not survive. \n",
    "# This means the 'Survived' column can be used as a target variable to analyze and compare the characteristics of survivors vs. non-survivors in the dataset.\n",
    "survi_mean = train.groupby(\"Survived\").mean(numeric_only=True)\n",
    "survi_0 = survi_mean.iloc[0,:] # avergage values of passenger that DID not survive\n",
    "survi_1 = survi_mean.iloc[1,:] # avergage values of passenger DID not survive\n",
    "# Relative difference calculation\n",
    "# This highlights which features differ most proportionally between the two groups, helping identify potential predictors of survival.\n",
    "abs((survi_1 - survi_0) / (survi_0 + survi_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce39a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relative differences show that Fare, Parch, and Pclass matter the most for survival.\n",
    "# Let's explore these features further to understand their impact,\n",
    "# which will help us gauge how much importance our XGBoost model should assign to them.\n",
    "# Fare by survival\n",
    "sns.violinplot(x=\"Fare\", hue=\"Survived\", data=train)\n",
    "plt.title(\"Passenger Fare by Survival\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pclass by survival\n",
    "sns.countplot(x=\"Pclass\", hue=\"Survived\", data=train)\n",
    "plt.title(\"Passenger Class by Survival\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe94cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parch by survival\n",
    "sns.violinplot(x=\"Parch\", hue=\"Survived\", data=train)\n",
    "plt.title(\"Passenger Parch by Survival\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For context, let's see the PassengerId value impact so we can see how little it matters\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.scatter(train[\"PassengerId\"], train[\"Survived\"], alpha=0.3)\n",
    "plt.xlabel(\"PassengerId\")\n",
    "plt.ylabel(\"Survived\")\n",
    "plt.title(\"PassengerId vs Survived\")\n",
    "plt.yticks([0,1])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a6fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix for all numeric features in the dataset.\n",
    "# Then plot a heatmap to visually inspect the strength and direction of linear relationships between pairs of numeric variables.\n",
    "# The heatmap's colors and annotated values help identify which features are strongly\n",
    "# positively or negatively correlated, which is useful for feature selection and understanding data structure.\n",
    "corr = train.select_dtypes(include=\"number\").corr()\n",
    "plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing this, we can check that, for example, the correlation with Survival is:\n",
    "# -0.005 with PassengerID, Almost no correlation, meaning that it has practically no correlation\n",
    "# 1 with Survived, perfect correlation with itself, for obvious reasons\n",
    "# -0.34 with Pclass, Moderate negative correlation, higher class (lower number) means MORE likely to survive\n",
    "# -0.077 with Age, Weak negative correlation, younger passengers slightly more likely to survive\n",
    "# -0.035 with SibSp, Weak negative correlation, having siblings/spouses means almost nothing when it comes to survive\n",
    "# 0.082 with Parch, Weak positive correlation, having parents/childrens means almost nothing when it comes to survive\n",
    "# 0.26 with Fare, Moderate positive correlation, paying higher fares increases chance of survival+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a411e",
   "metadata": {},
   "source": [
    "### Preprocessing data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[\"Name\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title using regex\n",
    "train[\"title\"] = train[\"Name\"].str.extract(r\",\\s*([^\\.\\\"]+)\\.\", expand=False)\n",
    "\n",
    "# Map titles to numeric codes using a dictionary\n",
    "title_mapping = {\n",
    "    \"Mr\": 1,\n",
    "    \"Master\": 3,\n",
    "    \"Ms\": 4, \"Mlle\": 4, \"Miss\": 4,\n",
    "    \"Mme\": 5, \"Mrs\": 5\n",
    "}\n",
    "\n",
    "# Map titles, assign 2 to all others (including unknown)\n",
    "train[\"title\"] = train[\"title\"].map(title_mapping).fillna(2).astype(int)\n",
    "\n",
    "print(train[\"title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Fare to True if above average, else False\n",
    "meanfare = train[\"Fare\"].mean()\n",
    "train[\"Fare\"] = train[\"Fare\"] > meanfare\n",
    "print(train[\"Fare\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With that, lets start from droping some values we don't need.\n",
    "train = train.drop([\"PassengerId\",\"Name\",\"Ticket\"], axis=\"columns\")\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets replace the values of sex with numbers so its easier for the model to work with\n",
    "train[\"Sex\"] = train[\"Sex\"].replace([\"male\", \"female\"], [0,1])\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28135443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since cabin either is null or has a random alphanumeric value, lets make it a boolean\n",
    "train[\"Cabin\"] = train[\"Cabin\"].isna()\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ages = train.groupby(\"title\")[\"Age\"].agg(\"mean\")\n",
    "print(mean_ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from title to age to fill missing values\n",
    "title_age_map = {\n",
    "    1:32,\n",
    "    2:45,\n",
    "    3:7,\n",
    "    4:23,\n",
    "    5:35,\n",
    "}\n",
    "\n",
    "# Fill missing Age values based on title\n",
    "train[\"Age\"] = train[\"Age\"].fillna(train[\"title\"].map(title_age_map))\n",
    "\n",
    "# Fill any remaining missings using the mean\n",
    "mean_age =  train[\"Age\"].mean()\n",
    "train[\"Age\"] = train[\"Age\"].fillna(mean_age)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts categorical variables in the DataFrame into one-hot encoded columns, creating new binary columns for each category value,so machine learning models can process categorical data easily\n",
    "train = pd.get_dummies(train)\n",
    "# So now we do this again after the data treatment\n",
    "survi_mean = train.groupby(\"Survived\").mean()\n",
    "survi_0 = survi_mean.iloc[0,:] \n",
    "survi_1 = survi_mean.iloc[1,:] \n",
    "abs((survi_1 - survi_0) / (survi_0 + survi_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8521b5",
   "metadata": {},
   "source": [
    "### Modeling ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ded65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Survived column from train and assign it to y (target variable), leaving train with only feature columns which we assign to x (input features)\n",
    "y = train[\"Survived\"]\n",
    "x = train.drop(columns=[\"Survived\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic XGBoost classifier usage\n",
    "model = XGBClassifier()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10)\n",
    "model.fit(x_train, y_train)\n",
    "print(model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the prediction the model does for survivors\n",
    "pred = model.predict(x_test)\n",
    "print(\"Survived\", sum(pred != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f135fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the prediction the model does for non-survivors\n",
    "pred = model.predict(x_test)\n",
    "print(\"Not Survived\", sum(pred == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e08ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a confusion matrix to check how many TN, FP, FN and TP we have.\n",
    "# [[TN, FP],   <-- First row: True Negatives (TN), False Positives (FP)\n",
    "#  [FN, TP]]   <-- Second row: False Negatives (FN), True Positives (TP)\n",
    "conmatrix = confusion_matrix(y_test, pred)\n",
    "conmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93745548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We obtained 86% accuracy with XGBoost model, now we might want to adjust its hyperparameter using GridSearch\n",
    "cv_splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "model = XGBClassifier(random_state=42)  # fixes XGBoost's own RNG\n",
    "ran_grid = {\"eta\": np.linspace(0, 0.5, num=12)}\n",
    "\n",
    "ran = GridSearchCV(model, ran_grid, cv=cv_splitter)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=11\n",
    ")\n",
    "\n",
    "ran.fit(x_train, y_train)\n",
    "print(ran.best_score_)\n",
    "print(ran.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We adjust max_depth\n",
    "model = XGBClassifier(eta = 0.045454545454545456)\n",
    "random_grid = {\"max_depth\": range(1,20,1)}\n",
    "\n",
    "ran = GridSearchCV(model, random_grid, cv=5)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=11)\n",
    "ran.fit(x_train, y_train)\n",
    "print(ran.best_score_)\n",
    "ran.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be665341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We adjust min_child_weight\n",
    "model = XGBClassifier(eta = 0.045454545454545456, max_depth = 6)\n",
    "random_grid = {\"min_child_weight\": range(1,20,1)}\n",
    "\n",
    "ran = GridSearchCV(model, random_grid, cv=5)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=11)\n",
    "ran.fit(x_train, y_train)\n",
    "print(ran.best_score_)\n",
    "ran.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a5f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We adjust gamma\n",
    "model = XGBClassifier(eta = 0.045454545454545456, max_depth = 6, min_child_weight=1)\n",
    "random_grid = {\"gamma\" : [i / 10.0 for i in range(0, 11)]}\n",
    "\n",
    "ran = GridSearchCV(model, random_grid, cv=5)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=11)\n",
    "ran.fit(x_train, y_train)\n",
    "print(ran.best_score_)\n",
    "ran.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4017898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We adjust subsample\n",
    "model = XGBClassifier(eta=0.045454545454545456, max_depth=6, min_child_weight=1, gamma=0.3)\n",
    "random_grid = {\"subsample\": [i / 100.0 for i in range(50, 101, 5)]}\n",
    "\n",
    "ran = GridSearchCV(model, random_grid, cv=5)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=11)\n",
    "ran.fit(x_train, y_train)\n",
    "print(ran.best_score_)\n",
    "print(ran.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e476ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We adjust comsample_bytree\n",
    "model = XGBClassifier(eta=0.045454545454545456, max_depth=6, min_child_weight=1, gamma=0.3, subsample=1.0)\n",
    "random_grid = {\"colsample_bytree\": [i / 10.0 for i in range(5, 11)]}  # 0.5 to 1.0\n",
    "\n",
    "ran = GridSearchCV(model, random_grid, cv=5)\n",
    "ran.fit(x_train, y_train)\n",
    "print(ran.best_score_)\n",
    "print(ran.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa0240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We adjust reg_alpha\n",
    "model = XGBClassifier(\n",
    "    eta=0.045454545454545456,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.3,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0\n",
    ")\n",
    "random_grid = {\"reg_alpha\": [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]}\n",
    "\n",
    "ran = GridSearchCV(model, random_grid, cv=5)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=11)\n",
    "ran.fit(x_train, y_train)\n",
    "print(ran.best_score_)\n",
    "print(ran.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439665ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check how we doing isn 92,17% accuracy\n",
    "model = XGBClassifier(\n",
    "    eta=0.045454545454545456,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.3,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    reg_alpha = 0.05\n",
    ")\n",
    "\n",
    "model.fit(x, y)\n",
    "print(model.score(x_test, y_test))\n",
    "pred = model.predict(x_test)\n",
    "print(\"Survived\", sum(pred!=0))\n",
    "print(\"Not Survived\", sum(pred==0))\n",
    "\n",
    "conmatrix = confusion_matrix(y_test, pred)\n",
    "conmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce5d57f",
   "metadata": {},
   "source": [
    "We trained the XGBoost model with the best hyperparameters found via GridSearchCV\n",
    "We achieve 92.17% accuracy on the test set (x_test, y_test)\n",
    "Then we predict the 'Survived' labels and print how many passengers the model predicts survived and not survived\n",
    "Finally, we compute the confusion matrix to evaluate performance in detail:\n",
    " - 113 true negatives (correctly predicted not survived)\n",
    " - 52 true positives (correctly predicted survived)\n",
    " - 5 false positives (predicted survived but actually did not survive)\n",
    " - 9 false negatives (predicted not survived but actually survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "model_path = os.path.join(ROOT, \"models\", \"titanic_xgb_92_18.pkl\")\n",
    "data_path = os.path.join(ROOT, \"data\", \"processed_titanic.csv\")\n",
    "\n",
    "joblib.dump(model, model_path)\n",
    "# Save the fully processed dataset for testing or deployment\n",
    "train.to_csv(data_path, index=False)\n",
    "\n",
    "# print(\"Model saved to:\", model_path)\n",
    "# print(\"Data saved to:\", data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
